---
title: 'Performance Penalty from False Sharing in Parallel Programs'
author: ~
date: '2025-02-01'
slug: false-sharing
categories: []
tags: []
description: 'False sharing is a performance penalty that occurs when multiple threads modify variables that are close to each other in memory.'
draft: true
output:
  blogdown::html_page:
    toc: false
---



<p>In concurrent programming, a variable may be shared between threads.
To prevent race conditions, threads must synchronize their access to the variable.
While one thread is using the variable, it assumes exclusive ownership
and other threads must wait until it is done.</p>
<p>False sharing occurs when a variable is <em>not</em> logically shared with any other thread,
but other threads still have to wait on a thread that is modifying the variable.</p>
<div id="cache-coherency" class="section level2">
<h2>Cache Coherency</h2>
<p>Each CPU in a processor has a unique cache, a small amount of memory used to speed up access to data.
When data is shared between CPUs,
the cache coherence protocol ensures that all CPUs see the same value for the variable.
The smallest unit of data synchronized between caches is called a cache line,
which is typically 64 bytes (128 bytes on M-series Apple CPUs).
Meaning, if any data in a cache line is modified, the entire cache line must be synchronized.</p>
<p>This creates a problem when two different CPUs are working on two different variables
that are in the same cache line. Each time they operate on that variable,
they have to wait for the cache line to be synchronized.</p>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<p>It’s surprisingly easy to create false sharing,
because compilers typically want to pack data together in memory
for performance and size.</p>
<p>For example, this totally normal variable declaration is vulnerable to false sharing:</p>
<pre class="c"><code>uint16_t a = 0;
uint16_t b = 0;</code></pre>
<p><code>a</code> and <code>b</code> are both 2 byte variables and they are adjacent in memory
since they are declared right next to each other.
Unless <code>a</code> just happened to be in the last two bytes of a cache line,
the two variables will be in the same cache line.
Anytime one is modified, the cache line must be synchronized causing
another thread to wait.</p>
<p>For example, this OpenMP parallel program will have false sharing:</p>
<pre class="c"><code>#pragma omp parallel num_threads(2)
{
    int id = omp_get_thread_num();
    do_work(id == 0 ? &amp;a : &amp;b);
}</code></pre>
<p>The <a href="https://github.com/tmastny/cache/blob/1565c00bbee5d3a1891e008e858015d4463f61f8/false-sharing/ab.c#L25-L31">solution</a>
is to pad the variables so they are in different cache lines.</p>
<pre class="c"><code>uint16_t a = 0;
char padding[CACHE_LINE_SIZE - sizeof(uint16_t)];
uint16_t b = 0;</code></pre>
<p>It seems a little wasteful, but depending on the workload,
but avoiding false sharing significantly improve performance.</p>
</div>
<div id="performance" class="section level2">
<h2>Performance</h2>
<p>The impact of the previous example is small:
false sharing slows down the program by about 0.6%.</p>
<p>But we can find more dramatic examples.
Before, two CPUs had to sync on the same cache line,
but my M3 MacBook Air has 8 cores.</p>
<p>An array of 8 <code>uint16_t</code> is only 16 bytes,
which easily fits on any modern cache line.</p>
<pre class="c"><code>uint16_t counters[8];</code></pre>
<p>If 8 different threads are writing to different elements of the array,
they will have to wait for each other to finish modifying the cache line.</p>
<pre class="c"><code>#pragma omp parallel
{
    int id = omp_get_thread_num();
    do_work(&amp;counters[id]);
}</code></pre>
<p>We can avoid this again by padding. There’s a couple of different ways to do this.
First, is to create a larger array and index into the array at <code>CACHE_LINE_SIZE</code> multiples.</p>
<pre class="c"><code>uint16_t counters[8 * CACHE_LINE_SIZE];
// ...
do_work(&amp;counters[id * CACHE_LINE_SIZE]);</code></pre>
<p>A more general approach is to use a struct with padding.</p>
<pre class="c"><code>struct counter {
    uint16_t value;
    char padding[CACHE_LINE_SIZE - sizeof(uint16_t)];
};
struct counter counters[8];
// ...
do_work(&amp;counters[id].value);</code></pre>
<p>Here we see a significant performance improvement:
the padded version is 25% faster than the unpadded version.</p>
<div id="architecture-dependent-performance" class="section level3">
<h3>Architecture Dependent Performance</h3>
<p>One unique feature of the M-series Apple processors
is that they both Performance (P) and Efficiency (E) cores.
The P cores have independent L1 caches, but share an L2 cache
and have faster cache interconnects.
Cache interconnections are how cache lines are synchronized between CPUs.</p>
<p>There are likely cache interconnects between P and E cores,
but they cores are physically further away and the interconnects
may have lower bandwidth. We can see this effect when we measure
the impact of false sharing by the number of cores exposed to false sharing.</p>
<p>In the counter example above, we have a discontinuous jump after 4 cores:
that’s when the cache lines have to synchronize between P and E cores.</p>
<table>
<thead>
<tr class="header">
<th>Threads</th>
<th>Regular</th>
<th>128-byte Padded</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>0.019</td>
<td>0.018</td>
<td>5.06%</td>
</tr>
<tr class="even">
<td>4</td>
<td>0.021</td>
<td>0.019</td>
<td>7.97%</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.028</td>
<td>0.020</td>
<td>25.84%</td>
</tr>
<tr class="even">
<td>8</td>
<td>0.030</td>
<td>0.023</td>
<td>25.12%</td>
</tr>
</tbody>
</table>
<p>The impact of syncing across P and E cores is comically large
when working with atomic operations:</p>
<table>
<thead>
<tr class="header">
<th>Threads</th>
<th>Regular</th>
<th>128-byte Padded</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>1.362</td>
<td>0.189</td>
<td>620.1%</td>
</tr>
<tr class="even">
<td>4</td>
<td>4.201</td>
<td>0.193</td>
<td>2079.8%</td>
</tr>
<tr class="odd">
<td>6</td>
<td>18.735</td>
<td>0.210</td>
<td>8821.5%</td>
</tr>
<tr class="even">
<td>8</td>
<td>20.378</td>
<td>0.227</td>
<td>8890.4%</td>
</tr>
</tbody>
</table>
<p>Programmatically, the atomic operation is very similar to the normal counter version:</p>
<pre class="c"><code>#include &lt;stdatomic.h&gt;

struct counter { atomic_int value; };
struct counter counters[NUM_THREADS];

#pragma omp parallel
{
    int id = omp_get_thread_num();
    pin_to_core(id);  // Pin to P cores 0-3
    
    for(int i=0; i&lt;ITERATIONS; i++)
        atomic_fetch_add_explicit(&amp;counters[id].value, 1, memory_order_relaxed);
}</code></pre>
<p>But atomic operations have much more strigent requirements for cache coherency.
Every single increment from any of the CPU must obtain exclusive access, write,
and then release.</p>
<p>The normal counter version must still enforce cache coherency when another CPU
wants to write to an invalid cache line,
but features such as the <a href="https://en.wikipedia.org/wiki/Write_combining">writing-combine buffer</a>
means that the CPU may not have to synchronize on every single write: some writes can
be batched and applied together.</p>
</div>
</div>
<div id="false-sharing" class="section level2">
<h2>False sharing</h2>
<p>False sharing!</p>
<p>Recall cache coherency policies:
* CPU-0 writes to a byte in a cache line.
* that cache line is marked as M, modified.
* the memory controller marks that cache line as I, invalid
for all other CPUs.
* Later, CPU-1 reads the same cache line and sees it as invalid.
* the memory controll then requests the cache line from
CPU-0 and both are marked as S for shared.</p>
<p>Problem: two separate threads working on independent pieces of
data in the same cache line. Even though there is no mutex sync,
they have to wait for the cache coherency policy each write.
Example:
* thread A writes to cache line, becomes M
* thread B reads the same cache line sees line as I
* thread B has to wait for the cache line to be synced
from CPU-0 to CPU-1</p>
<p>Assume that a mutex is 40 bytes. One idea is to pad the mutexes,
so each is on a separate cache line. So instead of 16 * 40 = 640 bytes,
we use 16 * 64 = 1024 bytes, but each mutex is separate,
separate threads locking on different mutexes never have to wait for
cache coherency.</p>
<p>One interesting note: if we used <code>rowlock</code> how many mutexes would we get for
1024 bytes? 1024 / 40 = 25. So the extra memory from padding isn’t even that much,
especially since those mutexes could still be false sharing.</p>
</div>
