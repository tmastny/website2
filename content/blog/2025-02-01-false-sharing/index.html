---
title: 'Performance Penalty from False Sharing in Parallel Programs'
author: ~
date: '2025-02-01'
slug: false-sharing
categories: []
tags: []
description: 'False sharing is a performance penalty that occurs when multiple threads modify variables that are close to each other in memory.'
draft: true
output:
  blogdown::html_page:
    toc: false
---



<p>In concurrent programming, a variable may be shared between threads.
To prevent race conditions, threads must synchronize their access to the variable.
While one thread is using the variable, it assumes exclusive ownership
and other threads must wait until it is done.</p>
<p>False sharing occurs when a variable is <em>not</em> logically shared with any other thread,
but other threads still have to wait on a thread that is modifying the variable.</p>
<div id="cache-coherency" class="section level2">
<h2>Cache Coherency</h2>
<p>Each CPU in a processor has a unique cache, a small amount of memory used to speed up access to data.
When data is shared between CPUs,
the cache coherence protocol ensures that all CPUs see the same value for the variable.
The smallest unit of data synchronized between caches is called a cache line,
which is typically 64 bytes (128 bytes on M-series Apple CPUs).
Meaning, if any data in a cache line is modified, the entire cache line must be synchronized.</p>
<p>This creates a problem when two different CPUs are working on two different variables
that are in the same cache line. Each time they operate on that variable,
they have to wait for the cache line to be synchronized.</p>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<p>It’s surprisingly easy to create false sharing,
because compilers typically want to pack data together in memory
for performance and size.</p>
<p>For example, this totally normal variable declaration is vulnerable to false sharing:</p>
<pre class="c"><code>uint16_t a = 0;
uint16_t b = 0;</code></pre>
<p><code>a</code> and <code>b</code> are both 2 byte variables and they are adjacent in memory
since they are declared right next to each other.
Unless <code>a</code> just happened to be in the last two bytes of a cache line,
the two variables will be in the same cache line.
Anytime one is modified, the cache line must be synchronized causing
another thread to wait.</p>
<p>For example, this OpenMP parallel program will have false sharing:</p>
<pre class="c"><code>#pragma omp parallel num_threads(2)
{
    int id = omp_get_thread_num();
    do_work(id == 0 ? &amp;a : &amp;b);
}</code></pre>
<p>The <a href="https://github.com/tmastny/cache/blob/1565c00bbee5d3a1891e008e858015d4463f61f8/false-sharing/ab.c#L25-L31">solution</a>
is to pad the variables so they are in different cache lines.</p>
<pre class="c"><code>uint16_t a = 0;
char padding[CACHE_LINE_SIZE - sizeof(uint16_t)];
uint16_t b = 0;</code></pre>
<p>It seems a little wasteful, but depending on the workload,
but avoiding false sharing significantly improve performance.</p>
</div>
<div id="performance" class="section level2">
<h2>Performance</h2>
<p>The impact of the previous example is small:
false sharing slows down the program by about 0.6%.</p>
<p>But we can find more dramatic examples.
Before, two CPUs had to sync on the same cache line,
but my M3 MacBook Air has 8 cores.</p>
<p>An array of 8 <code>uint16_t</code> is only 16 bytes,
which easily fits on any modern cache line.</p>
<pre class="c"><code>uint16_t counters[8];</code></pre>
<p>If 8 different threads are writing to different elements of the array,
they will have to wait for each other to finish modifying the cache line.</p>
<pre class="c"><code>#pragma omp parallel
{
    int id = omp_get_thread_num();
    do_work(&amp;counters[id]);
}</code></pre>
<p>We can avoid this again by padding. There’s a couple of different ways to do this.
First, is to create a larger array make sure thread’s index into the array</p>
<pre class="c"><code>uint16_t counters[8 * CACHE_LINE_SIZE];
// ...
do_work(&amp;counters[id * CACHE_LINE_SIZE]);</code></pre>
<p>A more general approach is to use a struct with padding.</p>
<pre class="c"><code>struct counter {
    uint16_t value;
    char padding[CACHE_LINE_SIZE - sizeof(uint16_t)];
};
struct counter counters[8];
// ...
do_work(&amp;counters[id].value);</code></pre>
<p>Here we see a significant performance improvement:
the padded version is 25% faster than the unpadded version.</p>
</div>
<div id="false-sharing" class="section level2">
<h2>False sharing</h2>
<p>False sharing!</p>
<p>Recall cache coherency policies:
* CPU-0 writes to a byte in a cache line.
* that cache line is marked as M, modified.
* the memory controller marks that cache line as I, invalid
for all other CPUs.
* Later, CPU-1 reads the same cache line and sees it as invalid.
* the memory controll then requests the cache line from
CPU-0 and both are marked as S for shared.</p>
<p>Problem: two separate threads working on independent pieces of
data in the same cache line. Even though there is no mutex sync,
they have to wait for the cache coherency policy each write.
Example:
* thread A writes to cache line, becomes M
* thread B reads the same cache line sees line as I
* thread B has to wait for the cache line to be synced
from CPU-0 to CPU-1</p>
<p>Assume that a mutex is 40 bytes. One idea is to pad the mutexes,
so each is on a separate cache line. So instead of 16 * 40 = 640 bytes,
we use 16 * 64 = 1024 bytes, but each mutex is separate,
separate threads locking on different mutexes never have to wait for
cache coherency.</p>
<p>One interesting note: if we used <code>rowlock</code> how many mutexes would we get for
1024 bytes? 1024 / 40 = 25. So the extra memory from padding isn’t even that much,
especially since those mutexes could still be false sharing.</p>
</div>
