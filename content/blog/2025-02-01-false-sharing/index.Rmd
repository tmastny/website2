---
title: 'Performance Penalty from False Sharing in Parallel Programs'
author: ~
date: '2025-02-01'
slug: false-sharing
categories: []
tags: []
description: 'False sharing is a performance penalty that occurs when multiple threads modify variables that are close to each other in memory.'
draft: true
output:
  blogdown::html_page:
    toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE, message = FALSE,
  results = "show", cache = FALSE, autodep = FALSE, error = TRUE,
  echo = FALSE, fig.path = "figure-html/"
)
```

Which program is faster? 

<div class="row">
<div class="col-md-6">
```c
uint16_t a = 0;
uint16_t b = 0;
char c[128];

#pragma omp parallel num_threads(2)
{
    int id = omp_get_thread_num();
    do_work(id == 0 ? &a : &b);
}
```
</div>
<div class="col-md-6">
```c
uint16_t a = 0;
char c[128];
uint16_t b = 0;

#pragma omp parallel num_threads(2)
{
    int id = omp_get_thread_num();
    do_work(id == 0 ? &a : &b);
}
```
</div>
</div>




The second version! 


In concurrent programming, a variable may be shared between threads. 
To prevent race conditions, threads must synchronize their access to the variable.
While one thread is using the variable, it assumes exclusive ownership 
and other threads must wait until it is done.

False sharing occurs when a variable is _not_ logically shared with any other thread,
but other threads still have to wait on a thread that is modifying the variable. 

## Cache Coherency

Each CPU in a processor has a unique cache, a small amount of memory used to speed up access to data.
When data is shared between CPUs, 
the cache coherence protocol ensures that all CPUs see the same value for the variable.
The smallest unit of data synchronized between caches is called a cache line, 
which is typically 64 bytes (128 bytes on M-series Apple CPUs). 
If any data in a cache line is modified, the entire cache line must be synchronized.

This creates a problem when two different CPUs are working on two different variables
that are in the same cache line. Each time they operate on that variable, 
they have to wait for the cache line to be synchronized.

## Examples

It's surprisingly easy to create false sharing, 
because compilers typically want to pack data together in memory 
for performance and size. 

For example, this totally normal variable declaration is vulnerable to false sharing:

```c
uint16_t a = 0;
uint16_t b = 0;
```

`a` and `b` are both 2 byte variables and they are adjacent in memory
since they are declared right next to each other. 
Unless `a` just happened to be in the last two bytes of a cache line,
the two variables will be in the same cache line.
Anytime one is modified, the cache line must be synchronized causing
another thread to wait. 

For example, this OpenMP parallel program will have false sharing:

```c
#pragma omp parallel num_threads(2)
{
    int id = omp_get_thread_num();
    do_work(id == 0 ? &a : &b);
}
```

The [solution](https://github.com/tmastny/cache/blob/1565c00bbee5d3a1891e008e858015d4463f61f8/false-sharing/ab.c#L25-L31)
is to pad the variables so they are in different cache lines.

```c
uint16_t a = 0;
char padding[CACHE_LINE_SIZE - sizeof(uint16_t)];
uint16_t b = 0;
```

It seems a little wasteful, but depending on the workload,
but avoiding false sharing significantly improve performance.

## Performance

The impact of the previous example is small: 
false sharing slows down the program by about 0.6%. 

But we can find more dramatic examples. 
Before, two CPUs had to sync on the same cache line,
but my M3 MacBook Air has 8 cores. 

An array of 8 `uint16_t` is only 16 bytes, 
which easily fits on any modern cache line. 

```c
uint16_t counters[8];
```

If 8 different threads are writing to different elements of the array,
they will have to wait for each other to finish modifying the cache line.
```c
#pragma omp parallel
{
    int id = omp_get_thread_num();
    do_work(&counters[id]);
}
```

We can avoid this again by padding. There's a couple of different ways to do this.
First, is to create a larger array and index into the array at `CACHE_LINE_SIZE` multiples.

```c
uint16_t counters[8 * CACHE_LINE_SIZE];
// ...
do_work(&counters[id * CACHE_LINE_SIZE]);
```

A more general approach is to use a struct with padding.

```c
struct counter {
    uint16_t value;
    char padding[CACHE_LINE_SIZE - sizeof(uint16_t)];
};
struct counter counters[8];
// ...
do_work(&counters[id].value);
```

Here we see a significant performance improvement: 
the padded version is 25% faster than the unpadded version.

### Architecture Dependent Performance

One unique feature of the M-series Apple processors 
is the use of Performance (P) and Efficiency (E) cores.
In addition to larger caches and faster clock speeds,
P cores have faster cache interconnects between each other.
Cache interconnections are how cache lines are synchronized between CPUs.

There are likely cache interconnects between P and E cores, 
but they cores are physically further away and the interconnects 
may have lower bandwidth. We can see this effect when we measure
false sharing that crosses P and E cores.

In the counter example above, we have a discontinuous jump after 4 cores:
that's when the cache lines have to synchronize between P and E cores.

Threads | Regular | 128-byte Padded | Improvement
--------|---------|-----------------|-------------
2       | 0.019   | 0.018           | 5.06%
4       | 0.021   | 0.019           | 7.97%
6       | 0.028   | 0.020           | 25.84%
8       | 0.030   | 0.023           | 25.12%

The impact of false sharing across P and E cores is comically large
when working with atomic operations:

Threads | Regular | 128-byte Padded | Improvement
--------|---------|-----------------|-------------
2       | 1.362   | 0.189           | 620.1%
4       | 4.201   | 0.193           | 2079.8%
6       | 18.735  | 0.210           | 8821.5%
8       | 20.378  | 0.227           | 8890.4%

Programmatically, the atomic operation is very similar to the normal counter version: 
```c
#include <stdatomic.h>

struct counter { atomic_int value; };
struct counter counters[NUM_THREADS];

#pragma omp parallel
{
    int id = omp_get_thread_num();
    for(int i=0; i<ITERATIONS; i++)
        atomic_fetch_add_explicit(&counters[id].value, 1, memory_order_relaxed);
}
```

But atomic operations have much more strigent requirements for cache coherency.
In particular, they enforce sequential consistency
which means every single atomic instruction from any of the CPU 
must request exclusive access to the cache line, obtain it, write,
and then release. 
This cost increases with the number of cores^[
    See the _Cache line contention_ section of this [blog post](https://fgiesen.wordpress.com/2014/08/18/atomics-and-contention/)
    by Fabian Giesen for more details.
] and apparently increases between P and E cores in this particular architecture.

At first I thought that the normal counter version had to enforce the same level of cache coherency,
but in reality, non-atomic operations are much more lax. 
As Fabian Giesen explains in his [blog post](https://fgiesen.wordpress.com/2014/07/07/cache-coherency/),
caches don't respond to messages immediately, cores execute instructions out of order,
and writes can be batched together in a store buffer before being written to the cache.
This means the normal counter version has false sharing, 
but the synchronization does not happen on every single write. 

## False Sharing Mutexes

Remember anytime we define variables next to each other,
they might be susceptible to false sharing.
One particular case of this I find amusing is mutexes.

The entire point of a mutex is to synchronize access to a shared resource.
But they are often defined next to each other in memory
at the top of a `.c` file, meaning they are susceptible to false sharing!

```c
pthread_mutex_t mutex1;
pthread_mutex_t mutex2;
```

On my M3 MacBook Air, the size of `pthread_mutex_t` is 64 bytes,
possibly putting them in the same cache line. 

A practical example of this is creating a parallel-friendly 
hash table. You don't want to have a single mutex per hash table,
because then you would have no parallelism. But a hash per entry
would cost way too much memory.
A good compromise is to assign a fixed number of mutexes, say 16,
and index into them using the hash modulo 16.

```c
pthread_mutex_t mutexes[16];
```

Adjacent mutexes in the array are susceptible to false sharing.
The solution is to pad the mutexes so they are in different cache lines.

I compared the performance of the regular and padded array,
and the padded version only performed better when working across the P and E cores 
(in fact I got a small, but consistent penalty when using padding on just the P cores). 

Threads | Regular | 128-byte Padded | Improvement
--------|---------|-----------------|-------------
2       | 0.029   | 0.030          | -4.19%
4       | 0.030   | 0.032          | -4.04%
6       | 0.064   | 0.037          | 42.99%
8       | 0.072   | 0.042          | 41.90%

## Performance Loss Conditions

The performance loss from false sharing is most significant when: 

* there is a lot of cache line contention: in the counter examples,
  we have up to 8 threads bouncing the same cache line. In this case,
  only two cache lines can be false shared.
* the non-memory operations are extremely quick: in the previous examples,
  all we were doing was incrementing a counter. Here's we are locking and
  unlocking a mutex, which has a lot of overhead.

For reference:

Operation                | Approximate Cost
------------------------ | ----------------
Non-atomic Add           | ~1-3 cycles
Atomic Add               | ~5-50 cycles
Syscall Entry            | ~100-1000 cycles
Mutex Lock (uncontended) | ~200-2000 cycles
Context Switch           | ~1000-10000 cycles
Mutex Lock (contended)   | ~2000-20000 cycles

> * Uncontended lock: requires at least one syscall (lock)
> * Contended lock: requires syscall + context switch + another syscall when waking up

The mutexes have such a large overhead, they completely overshadow the false sharing,
or provide enough of a buffer with other instructions to avoid the performance loss.

The following example is a little contrived, but it is possible to consistently
see the impact of false sharing when working with mutexes if you try to acquire
multiple adjacent ones. 

```c
#pragma omp parallel num_threads(2)
{
    int id = omp_get_thread_num();
    int num_threads = omp_get_num_threads();
    
    // id = 0 takes even locks, id = 1 takes odd locks
    for(int i = 0; i < 10000000; i++) {
        for(int lock_index = id; lock_index < 16; lock_index += num_threads) {
            pthread_mutex_lock(&mutexes[lock_index]);
            do_work();
            pthread_mutex_unlock(&mutexes[lock_index]);
        }
    }
}
```

Threads | Regular | 128-byte Padded | Improvement
--------|---------|-----------------|-------------
2       | 0.367   | 0.367          | 0.22%
4       | 0.222   | 0.219          | 1.41%
6       | 0.335   | 0.197          | 41.18%
8       | 0.309   | 0.160          | 48.22%

## False sharing

False sharing!

Recall cache coherency policies:
* CPU-0 writes to a byte in a cache line.
* that cache line is marked as M, modified.
* the memory controller marks that cache line as I, invalid
  for all other CPUs.
* Later, CPU-1 reads the same cache line and sees it as invalid.
* the memory controll then requests the cache line from
  CPU-0 and both are marked as S for shared.

Problem: two separate threads working on independent pieces of
data in the same cache line. Even though there is no mutex sync,
they have to wait for the cache coherency policy each write.
Example:
* thread A writes to cache line, becomes M
* thread B reads the same cache line sees line as I
* thread B has to wait for the cache line to be synced
  from CPU-0 to CPU-1

Assume that a mutex is 40 bytes. One idea is to pad the mutexes,
so each is on a separate cache line. So instead of 16 * 40 = 640 bytes,
we use 16 * 64 = 1024 bytes, but each mutex is separate,
separate threads locking on different mutexes never have to wait for
cache coherency.

One interesting note: if we used `rowlock` how many mutexes would we get for
1024 bytes? 1024 / 40 = 25. So the extra memory from padding isn't even that much,
especially since those mutexes could still be false sharing.
