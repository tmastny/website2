---
title: 'Evaluating Branch Predictions'
author: ~
date: '2024-12-15'
slug: evaluating-branch-predictions
categories: []
tags: []
description: 'Evaluation Branch Predictions'
draft: true
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#algorithmic-analysis" id="toc-algorithmic-analysis">Algorithmic Analysis</a></li>
<li><a href="#reasons-to-think-gshare-is-better" id="toc-reasons-to-think-gshare-is-better">Reasons to think gshare is better</a>
<ul>
<li><a href="#gshare-is-better" id="toc-gshare-is-better">gshare is better</a></li>
<li><a href="#gshare-is-worse" id="toc-gshare-is-worse">gshare is worse</a></li>
</ul></li>
<li><a href="#mathematical-proof-that-gshare-is-worse" id="toc-mathematical-proof-that-gshare-is-worse">Mathematical proof that gshare is worse</a></li>
<li><a href="#proving-concatenation-has-fewer-key-collisions" id="toc-proving-concatenation-has-fewer-key-collisions">Proving concatenation has fewer key collisions</a>
<ul>
<li><a href="#collisions-in-a-random-program" id="toc-collisions-in-a-random-program">Collisions in a random program</a>
<ul>
<li><a href="#xor" id="toc-xor">XOR</a></li>
<li><a href="#concatenation" id="toc-concatenation">Concatenation</a></li>
<li><a href="#conclusion" id="toc-conclusion">Conclusion</a></li>
</ul></li>
</ul></li>
</ul>
</div>

<style>
    .simulation-container {
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 10px;
        margin: 10px 0;
    }

    .slider-container {
        width: 400px;
        padding: 0 10px;
    }

    input[type="range"] {
        width: 100%;
    }

    canvas {
        width: 400px;
        height: 120px;
    }
</style>

<script>
    function initBallBinSim(container) {
        const groupColors = [
            "#4CAF50", // green
            "#FF6B6B", // red
            "#45B7D1", // blue
            "#FFB347", // orange
        ];

        // Pre-generate 100 random states
        const states = Array(100)
            .fill()
            .map(() => {
                const xorBins = new Array(16).fill(0);
                const concatBins = new Array(16).fill(0);

                for (let i = 0; i < 16; i++) {
                    const addressGroup = Math.floor(i / 4);
                    xorBins[Math.floor(Math.random() * 16)]++;
                    concatBins[
                        addressGroup * 4 + Math.floor(Math.random() * 4)
                    ]++;
                }
                return { xorBins, concatBins };
            });

        function createBallBinViz(isXOR) {
            const canvas = document.createElement("canvas");
            const dpr = window.devicePixelRatio || 1;

            canvas.width = 400 * dpr;
            canvas.height = 120 * dpr;

            const ctx = canvas.getContext("2d");
            ctx.scale(dpr, dpr);

            function drawBins() {
                const numBins = 16;
                const binWidth = (400 - 20) / numBins;
                const binHeight = 70;
                const startY = 20;

                ctx.strokeStyle = "#333";
                ctx.lineWidth = 2;

                if (!isXOR) {
                    for (let group = 0; group < 4; group++) {
                        const x = 10 + group * (binWidth * 4);
                        ctx.fillStyle = groupColors[group] + "20";
                        ctx.fillRect(x, startY, binWidth * 4, binHeight);
                    }
                }

                for (let i = 0; i < numBins; i++) {
                    ctx.beginPath();
                    const x = 10 + i * binWidth;
                    ctx.moveTo(x, startY);
                    ctx.lineTo(x, startY + binHeight);
                    ctx.lineTo(x + binWidth, startY + binHeight);
                    ctx.lineTo(x + binWidth, startY);
                    ctx.stroke();
                }
            }

            function drawBall(binIndex, ballsInBin, addressGroup) {
                const numBins = 16;
                const binWidth = (400 - 20) / numBins;
                const startY = 20;
                const binHeight = 70;
                const ballRadius = 8;
                const ballSpacing = ballRadius * 2;

                const x = 10 + binIndex * binWidth + binWidth / 2;
                const y =
                    startY +
                    binHeight -
                    ballRadius -
                    ballSpacing * (ballsInBin - 1);

                ctx.beginPath();
                ctx.arc(x, y, ballRadius, 0, Math.PI * 2);
                ctx.fillStyle = isXOR ? "#4CAF50" : groupColors[addressGroup];
                ctx.fill();
                ctx.strokeStyle = "#333";
                ctx.stroke();
            }

            function draw(stateIndex) {
                ctx.clearRect(0, 0, 400, 120);
                drawBins();

                const bins = isXOR
                    ? states[stateIndex].xorBins
                    : states[stateIndex].concatBins;

                for (let i = 0; i < 16; i++) {
                    const addressGroup = Math.floor(i / 4);
                    for (let ball = 0; ball < bins[i]; ball++) {
                        drawBall(i, ball + 1, addressGroup);
                    }
                }
            }

            return { canvas, draw };
        }

        const simContainer = document.createElement("div");
        simContainer.className = "simulation-container";

        const xorViz = createBallBinViz(true);
        const concatViz = createBallBinViz(false);

        const sliderContainer = document.createElement("div");
        sliderContainer.className = "slider-container";

        const slider = document.createElement("input");
        slider.type = "range";
        slider.min = "0";
        slider.max = "99";
        slider.value = "0";

        slider.oninput = function () {
            const stateIndex = parseInt(this.value);
            xorViz.draw(stateIndex);
            concatViz.draw(stateIndex);
        };

        sliderContainer.appendChild(slider);

        simContainer.appendChild(xorViz.canvas);
        simContainer.appendChild(concatViz.canvas);
        simContainer.appendChild(sliderContainer);

        container.appendChild(simContainer);

        // Initial draw
        xorViz.draw(0);
        concatViz.draw(0);
    }

    document.addEventListener("DOMContentLoaded", function () {
        const containers = document.querySelectorAll(".ballbin-container");
        containers.forEach((container) => initBallBinSim(container));
    });
</script>
<blockquote>
<p>This is part of my branch prediction series.</p>
<ol style="list-style-type: decimal">
<li><a href="/blog/visualizing-cpu-pipelining/">Visualizng CPU Pipelining</a></li>
<li>Evaluating Branch Predictions</li>
<li><a href="/blog/hacking-lldb-to-evaluate-branch-predictions/">Hacking LLDB to Evaluate Branch Predictions</a></li>
</ol>
</blockquote>
<p>One way to extend the basic branch prediction algorithm
I cover in <a href="/blog/visualizing-cpu-pipelining/">Visualizing CPU Pipelining</a>
is to save the <em>history</em> of whether the last n branches were taken or not.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
The main advantage is that any consistent patterns in the history
can be used to predict the next branch.
For example, if the instruction pointer is on branch 4, there are two possible
histories: <code>NN</code> or <code>TT</code>. And based just on two-bits of history we can predict
the branch 4 with 100% accuracy.</p>
<pre><code>  a   h1  h2
              flag = False
  1   N   T   if cond1:
  2       T       if cond2:
                    flag = True
  3   N       elif cond3:
                  ...
&gt; 4   N   T   if flag:
                  ...</code></pre>
<p>One detail of this approach has particularly fascinated me:
how to combine the address and history effectively to form a key
into the branch prediction table. I’ll consider two approaches:</p>
<ul>
<li>concatenation: concatenates the last n/2 bits of the address and history</li>
<li>gshare: takes the XOR of the last n-bits of the address and history</li>
</ul>
<p><img src="concat-gshare.svg" /><!-- --></p>
<p>At first glance, it seems like gshare is the obvious choice
since it uses more information from both the address and history.
But how do we really know it’s better? Is this something we can mathematically prove?</p>
<p>In this post, I’ll share how I explored the differences between the two methods,
starting with algorithmic analysis more generally, examining concrete examples,
and even proving mathematically that gshare might be <strong>worse</strong> than concatenation!</p>
<div id="algorithmic-analysis" class="section level2">
<h2>Algorithmic Analysis</h2>
<p>In general, how do we compare any two algorithms?
For example, is quicksort better than insertion sort?
While quicksort has better average-case performance,
it has the same worst-case performance as insertion sort.
And insertion sort can even perform better when the list is “almost sorted”!</p>
<p>Another example is iterating over array of structs versus a
struct of arrays. In terms of computational complexity,
they are the same. But struct of arrays might be significantly
faster given the architecture of CPU cache memory.</p>
<p>Whether one algorithm is better than another depends on the
context and the assumptions you make when analyzing them.
So let’s make some assumptions and see how</p>
</div>
<div id="reasons-to-think-gshare-is-better" class="section level2">
<h2>Reasons to think gshare is better</h2>
<p>The basic argument why gshare is superior goes like this:
sometimes using more than n/2 address bits
would give us better predictions and sometimes using more of the branch history
would give us better predictions. The XOR hash in gshare uses more of both
by hashing all n-bits of the address and history into the key!</p>
<p>Compared concatenation where address and history have a predetermined
contribution to the key, gshare seems like an obvious<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> improvement.
We can even come up with some examples that demonstrate this argument.</p>
<div id="gshare-is-better" class="section level3">
<h3>gshare is better</h3>
<div id="same-branch-history-different-predictions" class="section level4">
<h4>Same branch history, different predictions</h4>
<p>With gshare, two different addresses with the same branch history
will always map to different keys: this is important
for prediction accuracy when the two branches have
different outcomes.</p>
<pre><code>Branch   address history  gshare  concat  actual
A        1010    1011     0001    1011    0
B        1110    1011     0100    1011    1</code></pre>
<p>gshare disambiguates the two branches
with different outcomes, while concatenation does not.</p>
</div>
<div id="using-more-history-bits" class="section level4">
<h4>Using more history bits</h4>
<p>Let’s assume we have a branch history pattern like this:</p>
<pre><code>1101 1101 1101 1101...</code></pre>
<p>With a 1-bit prediction table, long-term gshare has 100%
prediction accuracy, while concatenation has 50% accuracy.</p>
<pre><code>address history  gshare  concat  actual
1010    1101     0111    1001    1
1010    1110     0100    1000    1
1010    0111     1101    1011    0
1010    1011     1000    1011    1</code></pre>
<p>The reason is that the 0111 and 1011 histories alias.
Because the actual result alternates between 0 and 1,
the prediction bit alters as well.
Even if we used 2-bit saturation counter, we would only improve
the accuracy to 75%.</p>
</div>
</div>
<div id="gshare-is-worse" class="section level3">
<h3>gshare is worse</h3>
<p>However, we can also come up with an example where gshare
does <em>worse</em> than concatenation. Two keys in the branch
prediction table will collide in gshare
when the address of one equals the history of the other (and vice versa):</p>
<pre><code>Branch   Address  History  Gshare  Concat  Actual
A        0011     1100     0000    1100    1
B        1100     0011     0000    0011    0</code></pre>
<p>In this case, both branches map to the same prediction entry (0000)
even though they have different behaviors. Concatenation keeps them
separate because it uses different bits from the address and history.</p>
<p>So if the branch was running in a way where branch A was always executed
before branch B, the prediction accuracy of gshare would be 0%!</p>
<p>(TODO: let’s say we have best case scenario with separate branch histories,
even then gshare is zero. Worse)</p>
<pre><code>Branch Addr | History | Actual | Key  | Prediction
-----------------------------------------------------------------
0011       | 1001    |      1 | 1010 |         0
-----------------------------------------------------------------
1100       | 0110    |      0 | 1010 |         1
-----------------------------------------------------------------
0011       | 0011    |      0 | 0000 |         1
-----------------------------------------------------------------
1100       | 1100    |      1 | 0000 |         0
-----------------------------------------------------------------
0011       | 0110    |      0 | 0101 |         1
-----------------------------------------------------------------
1100       | 1001    |      1 | 0101 |         0
-----------------------------------------------------------------
0011       | 1100    |      1 | 1111 |         0
-----------------------------------------------------------------
1100       | 0011    |      0 | 1111 |         1
-----------------------------------------------------------------
0011       | 1001    |      1 | 1010 |         0
-----------------------------------------------------------------
1100       | 0110    |      0 | 1010 |         1
-----------------------------------------------------------------</code></pre>
<p>To me, this feels like a contrived example and ones
in the previous section feel like the typical case.
But how do we know? Can we prove it?</p>
</div>
</div>
<div id="mathematical-proof-that-gshare-is-worse" class="section level2">
<h2>Mathematical proof that gshare is worse</h2>
<p>Like quicksort vs. insertion sort, maybe we can write
a mathematical proof that gshare is better than concatenation.
Let’s come up with some assumptions and investigate.</p>
<p>Let’s define a “program” as a set branches, each with a unique n-bit address.
Each address has a random n-bit history.
One dimension of poor branch prediction performance is key collisions.</p>
<p>Can we show that gshare is less likely to have key collisions
than concatenation? <strong>No</strong>. In fact, the opposite is true.
gshare is slightly more likely to have key collisions
than concatenation! <a href="#appendix">See the appendix</a> for the proof
when the key size is 4-bits.</p>
<p>What’s going on here? We know gshare performs better in practice.
The problem is our assumptions. We assumed that the branch history
for each address is random. That might be reasonable assumption
when comparing sorting algorithms (although plenty of real world
data is almost sorted), but this is a really bad assumption for branch prediction.
Branch history is often very predictable, and even a simple algorithm
of “branch taken” can be 70% accurate!<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>Instead, we need to gather empirical data on branches taken,
analyze patterns, and compare algorithm performance.</p>
</div>
<div id="proving-concatenation-has-fewer-key-collisions" class="section level1">
<h1>Proving concatenation has fewer key collisions</h1>
<p>Before we dive into a mathematical proof,
here’s some intuition why concatenation is more likely
to have <em>fewer</em> key collisions.</p>
<p>Let’s start with the extreme case: what’s the maximum
number of key collisions?
For gshare, since we are assuming the history is random,
there a chance that each history maps to the inverse
of the address. That would mean each key would be <code>0000</code>
in the 4-bit case.
If this happens <em>all</em> keys would collide.</p>
<p>On the other hand, with a 4-bit concatenation scheme,
addresses with the same last 2 bits will always map
to the same set of 4 keys.</p>
<p><img src="addr-key.svg" /><!-- --></p>
<p>So any key can have at most 4 collisions.</p>
<p>But maximum number of collisions isn’t the full story.
The probability of 16 collisions is extremely low.
If we simulate 100 cases, XOR usually has no more than 4
(and rarely 5).
The next section will compare the average number
of collisions, not just the worst case.</p>
<div class="ballbin-container"></div>
<div id="collisions-in-a-random-program" class="section level2">
<h2>Collisions in a random program</h2>
<p>Let’s suppose we have a program consisting
of 16 unique branch addresses, each with a random 4-bit branch history.
And let’s suppose our branch prediction table has a fixed 4-bit key size,
allowing 16 different keys.</p>
<p>The surprising result is that the XOR
is more likely to have key collisions
than concatenation!</p>
<div id="xor" class="section level3">
<h3>XOR</h3>
<p>Our goal is to show that for 16 unique 4-bit addresses,
with a random 4-bit history, XOR will randomly map the addresses
to any of the 16 keys.</p>
<p>To start, we’ll first show that it’s possible to map any address
to any key.
Suppose <code>k</code> is any possible 4-bit key.
Then if we have a fixed 4-bit address <code>a</code>,
we can always find a 4-bit history <code>h</code> such that
<code>k = a ^ h</code>:</p>
<pre><code>k = 0 ^ k = (a ^ a) ^ k = a ^ (a ^ k) = a ^ h</code></pre>
<p>And for our address <code>a</code>, if two different histories
map to the same key, they are the same history:</p>
<pre><code>a ^ h1 =     k =      a ^ h2
    h1 = a ^ k = a ^ (a ^ h2) = h2</code></pre>
<p>Therefore, collisions must be possible, since
any address can map to any key and we are randomly
choosing the history.</p>
<p>Now let’s reframe the question into a standard combinatorial problem:
imagine our 16 unique
addresses are 16 different balls and our 16 keys are 16 different
bins. What’s the expected number of bins with 2 or more balls?</p>
<p>A good approach is to use the complement probability:
i.e. what’s the probability that a bin is empty
and what’s the probability that a bin has 1 ball?
Then the probability that a bin has 2 or more balls is</p>
<p><span class="math display">\[
1 - \left( \frac{15}{16} \right)^{16} - \binom{16}{1} \frac{1}{16} \left( \frac{15}{16} \right)^{15} \simeq 0.26
\]</span></p>
<p>Therefore, 16 * 0.26 = 4.23 bins are expected to have 2 or more balls.
So about 4 collisions are expected.</p>
</div>
<div id="concatenation" class="section level3">
<h3>Concatenation</h3>
<p>Now let’s think about concatenation.</p>
<p>This one is a little more complicated and also changes
depending on the concatenation scheme (for example,
we could use a concatenation ratio of 2:1 instead of 1:1).
But to show the general process, let’s just analyze the case
where we have 16 unique addresses with a random 4-bit history,
and we concatenate 2 bits from the address and 2 bits from the history
to form the key.</p>
<p>Because we are only using the last 2 bits of the address,
we can treat this problem as 4 separate cases of 4 balls
into 4 bins.</p>
<p>For example, the addresses that end in <code>00</code> can only be mapped
to the keys that start with <code>00</code>.</p>
<p>Using the same formula as before, the expected number of bins
with 2 or more balls is about 0.267 * 4 = 1.05.
Since there are 4 independent cases of the 4 balls, 4 bins,
the expected number of bins with 2 or more balls is about
4.19.</p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>So concatenation is less likely to have key collisions
than XOR! So why does gshare work better than concatenation
in practice? Because branch history is not random.
The only way to really know which performs better is to
analyze empirical branching data.</p>
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>
Check out Dan Luu’s <a href="https://danluu.com/branch-prediction/">post</a>
for a great overview of this and other branch prediction algorithms.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Turns out not be obvious.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>We are also making the assumption
that key collisions are an indicator
of poor branch prediction performance. That seems true, but who knows?
Maybe there’s a clever way to use key collisions to improve branch prediction,
by making sure keys that alias have correlated predictions.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
