---
title: 'Tuning and Cross-validation with tidymodels and scikit learn'
author: ~
date: '2020-06-30'
categories: []
tags: ['sklearn', 'tidymodels']
description:
draft: true
output:
  blogdown::html_page:
    toc: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#tidymodels-and-sklearn">tidymodels and sklearn</a></li>
</ul>
</div>

<style type="text/css">
body {
  max-width: 1000px;
}
</style>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><a href="https://www.tidymodels.org/">tidymodels</a> is the new framework from
Max Kuhn, David Vaughan, and Julia Silge at RStudio. It’s the successor to
the <a href="https://topepo.github.io/caret/">caret</a> package, which was heavily
featured in Max Kuhn’s book
<a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a>.</p>
<p>tidymodels promises a modular, extensible design for machine learning in R.
It also has a wonderful <a href="https://www.tidymodels.org/">website</a> design
to help you get started as soon as possible, with a focus on examples and
case studies walking you through the components of the platform.</p>
<p>In this article, I want to translate concepts between tidymodels and
scikit-learn, Python’s standard machine learning ecosystem. I hope this will
be useful for Python Data Scientists interested in R, and R users who want to
learn a little more about Python.</p>
<p>I’ll cover the key machine learning workflows with scikit-learn and tidymodels
side-by-side, starting with preprocessing and feature engineering,
model tranining and cross-validation, as well as evaluation and comparison.</p>
<p>I think you’ll find that while both frameworks have their own unique advantages,
there are many conceptual similarities that have helped me understand both
frameworks better.</p>
</div>
<div id="tidymodels-and-sklearn" class="section level2">
<h2>tidymodels and sklearn</h2>
<p>We’ll go through this tidymodels example:
<a href="https://www.tidymodels.org/start/tuning/">4 Tune Model Parameters</a>
side-by-sideline with sklearn.</p>
<p>Notebook saved here: <a href="https://github.com/tmastny/sageprocess/blob/master/vignettes/sklearn-tidymodels-example1.ipynb" class="uri">https://github.com/tmastny/sageprocess/blob/master/vignettes/sklearn-tidymodels-example1.ipynb</a></p>
</div>
<div class="row">
<div id="tidymodels" class="section level2 col-md-6">
<h2>tidymodels</h2>
<pre class="r"><code>library(tidymodels)
library(modeldata)
library(vip)
library(readr)

cells &lt;- read_csv(&quot;cells.csv&quot;)</code></pre>
</div>
<div id="sklearn" class="section level2 col-md-6">
<h2>sklearn</h2>
<pre class="python"><code>import pandas as pd
import numpy as np

np.random.seed(753)
cells = pd.read_csv(&quot;cells.csv&quot;)</code></pre>
</div>
</div>
Split raw data
<div class="row">
<div id="tidymodels-1" class="section level2 col-md-6">
<h2>tidymodels</h2>
<pre class="r"><code>cell_split &lt;- initial_split(
  cells,
  strata = class
)

cell_train &lt;- training(cell_split)
cell_test &lt;- testing(cell_split)</code></pre>
</div>
<div id="sklearn-1" class="section level2 col-md-6">
<h2>sklearn</h2>
<pre class="python"><code>from sklearn.model_selection import train_test_split

features = cells.drop(&#39;class&#39;, axis=1)
outcome = cells[&#39;class&#39;]

X_train, X_test, y_train, y_test = train_test_split(
    features, 
    outcome, 
    test_size=0.25, 
    stratify=outcome
)</code></pre>
</div>
</div>
<p>We have to do a little preprocessing. All the features are numeric,
but the outcome is binary. R expects categorical outcomes as factors.
Python automatically changes outcome strings with <code>LabelBinarizer</code>.
We also use the identity transformation <code>FunctionTransformer()</code> to remove
the <code>case</code> column by dropping it from selected columns.</p>
<div class="row">
<div id="tidymodels-2" class="section level2 col-md-6">
<h2>tidymodels</h2>
<pre class="r"><code>tree_rec &lt;- recipe(class ~ ., data = cells) %&gt;%
  step_rm(case) %&gt;%
  step_string2factor(all_outcomes())</code></pre>
</div>
<div id="sklearn-2" class="section level2 col-md-6">
<h2>sklearn</h2>
<pre class="python"><code>from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import make_column_transformer

tree_preprocess = make_column_transformer(
    (FunctionTransformer(), features.drop(&#39;case&#39;, axis=1).columns)
)</code></pre>
</div>
</div>
<p>With R, we chose the implementation using <code>set_engine</code> and <code>set_mode</code>.
For sklearn, if we wanted a different implementation we would import
a different module that implemented the sklearn interface.</p>
<div class="row">
<div class="col-md-6">
<pre class="r"><code>tree_model &lt;- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune()
) %&gt;%
  set_engine(&quot;rpart&quot;) %&gt;%
  set_mode(&quot;classification&quot;)</code></pre>
</div>
<div class="col-md-6">
<pre class="python"><code>from sklearn.tree import DecisionTreeClassifier</code></pre>
</div>
</div>
<p>Both frameworks have a way to combine all the components of the model.</p>
<div class="row">
<div class="col-md-6">
<pre class="r"><code>set.seed(345)

tree_wf &lt;- workflow() %&gt;%
  add_model(tree_model) %&gt;%
  add_recipe(tree_rec)</code></pre>
</div>
<div class="col-md-6">
<pre class="python"><code>from sklearn.pipeline import make_pipeline

tree_pipeline = make_pipeline(
    tree_preprocess,
    DecisionTreeClassifier()
)</code></pre>
</div>
</div>
<p>Next, we can come up with tuning grids. tidymodels has sensible defaults
for all parameters. With sklearn, you typically have to come up with them
yourself.</p>
<div class="row">
<div class="col-md-6">
<pre class="r"><code>tree_grid &lt;- grid_regular(
  cost_complexity(),
  tree_depth(),
  levels = 5
)

tree_grid</code></pre>
<pre><code>## # A tibble: 25 x 2
##    cost_complexity tree_depth
##              &lt;dbl&gt;      &lt;int&gt;
##  1    0.0000000001          1
##  2    0.0000000178          1
##  3    0.00000316            1
##  4    0.000562              1
##  5    0.1                   1
##  6    0.0000000001          4
##  7    0.0000000178          4
##  8    0.00000316            4
##  9    0.000562              4
## 10    0.1                   4
## # … with 15 more rows</code></pre>
</div>
<div class="col-md-6">
<pre class="python"><code>param_grid = {
    &#39;decisiontreeclassifier__max_depth&#39;: [1, 4, 8, 11, 15],
    &#39;decisiontreeclassifier__ccp_alpha&#39;: [
        0.0000000001, 0.0000000178, 0.00000316, 0.000562, 0.1
    ] 
}</code></pre>
</div>
</div>
<p>Next, we construct and run cross-validation. By default, tidymodels calculates
both accuracy and area under the ROC curve. We’ll need to add those manually
to sklearn’s metrics.</p>
<div class="row">
<div class="col-md-6">
<pre class="r"><code>set.seed(234)
cell_folds &lt;- vfold_cv(cell_train, v = 5)

tree_res &lt;- tree_wf %&gt;% 
  tune_grid(
    resamples = cell_folds,
    grid = tree_grid
  )</code></pre>
</div>
<div class="col-md-6">
<pre class="python"><code>from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score

tree_scorer = {
    &#39;roc_auc&#39;: make_scorer(roc_auc_score, needs_proba=True),
    &#39;accuray&#39;: make_scorer(accuracy_score)
}

tree_tuner = GridSearchCV(
    tree_pipeline, param_grid, cv=5, 
    scoring=tree_scorer,
    refit=&#39;roc_auc&#39;
)

tree_res = tree_tuner.fit(X_train, y_train)</code></pre>
</div>
</div>
<p>List training metrics and hyperparamterics.</p>
<div class="row">
<div class="col-md-6">
<pre class="r"><code>best_param &lt;- tree_res %&gt;%
  select_best(&quot;roc_auc&quot;)

best_param</code></pre>
<pre><code>## # A tibble: 1 x 2
##   cost_complexity tree_depth
##             &lt;dbl&gt;      &lt;int&gt;
## 1    0.0000000001          4</code></pre>
</div>
<div class="col-md-6">
<pre class="python"><code>tree_res.best_params_</code></pre>
<pre><code>## {&#39;decisiontreeclassifier__ccp_alpha&#39;: 1.78e-08, &#39;decisiontreeclassifier__max_depth&#39;: 4}</code></pre>
</div>
</div>
<div class="row">
<div class="col-md-6">
<pre class="r"><code>tree_res %&gt;%
  show_best(&quot;roc_auc&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 7
##   cost_complexity tree_depth .metric .estimator  mean     n std_err
##             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1    0.0000000001          4 roc_auc binary     0.844     5 0.0137 
## 2    0.0000000178          4 roc_auc binary     0.844     5 0.0137 
## 3    0.00000316            4 roc_auc binary     0.844     5 0.0137 
## 4    0.000562              4 roc_auc binary     0.844     5 0.0137 
## 5    0.0000000001          8 roc_auc binary     0.837     5 0.00811</code></pre>
</div>
<div class="col-md-6">
<pre class="python"><code>pd.DataFrame(tree_res.cv_results_) \
    .sort_values(&#39;mean_test_roc_auc&#39;, ascending=False) \
    .rename(columns={
        &#39;param_decisiontreeclassifier__ccp_alpha&#39;: &#39;cost&#39;,
        &#39;param_decisiontreeclassifier__max_depth&#39;: &#39;max_depth&#39;
    }) \
    [[
        &#39;cost&#39;, &#39;max_depth&#39;,
        &#39;mean_test_accuray&#39;, &#39;mean_test_roc_auc&#39;
    ]] \
    .head(5)</code></pre>
<pre><code>##         cost max_depth  mean_test_accuray  mean_test_roc_auc
## 6   1.78e-08         4           0.797240           0.839899
## 1      1e-10         4           0.796584           0.839053
## 11  3.16e-06         4           0.795257           0.837103
## 16  0.000562         4           0.795917           0.837055
## 0      1e-10         1           0.738461           0.779942</code></pre>
</div>
</div>
<p>Variable importance. This is where sklearn becomes significantly more difficult.
Column name metadata is not saved in the final output, so we have to
manually add column names back to an ordered list of feature importances.</p>
<div class="row">
<div class="col-md-6">
<pre class="r"><code>best_tree &lt;- tree_wf %&gt;%
  finalize_workflow(best_param) %&gt;%
  fit(data = cell_train)

best_tree %&gt;%
  pull_workflow_fit() %&gt;%
  vip()</code></pre>
<p><img src="/blog/2020-06-18-tidymodels-sklearn/index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
<div class="col-md-6">
<pre class="python"><code>best_tree = tree_res.best_estimator_.named_steps[&#39;decisiontreeclassifier&#39;]
ct = tree_res.best_estimator_.named_steps[&#39;columntransformer&#39;]

feature_importances = pd.DataFrame({&#39;name&#39;: ct.transformers_[0][2]}) \
    .assign(importance = best_tree.feature_importances_) \
    .sort_values(&#39;importance&#39;, ascending=False)
    
import seaborn as sns
sns.barplot(x=&#39;importance&#39;, y=&#39;name&#39;, data=feature_importances.head(10))</code></pre>
<p><img src="/blog/2020-06-18-tidymodels-sklearn/index_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
</div>
<p>Final validation</p>
<div class="row">
<div class="col-md-6">
<pre class="r"><code>best_tree %&gt;%
  last_fit(cell_split) %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.802
## 2 roc_auc  binary         0.859</code></pre>
</div>
<div class="col-md-6">
<pre class="python"><code>pd.DataFrame.from_records([
    (name, scorer(tree_res.best_estimator_, X_test, y_test)) 
    for name, scorer in tree_scorer.items()
], columns=[&#39;metric&#39;, &#39;score&#39;])</code></pre>
<pre><code>##     metric     score
## 0  roc_auc  0.824385
## 1  accuray  0.776238</code></pre>
</div>
</div>
