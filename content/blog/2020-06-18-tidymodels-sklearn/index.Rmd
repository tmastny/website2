---
title: 'Tuning and Cross-validation with tidymodels and scikit learn'
author: ~
date: '2020-06-18'
categories: []
tags: []
description:
draft: true
output:
  blogdown::html_page:
    toc: true
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,
                      results='show', cache=FALSE, autodep=FALSE)
```

```{css, echo=FALSE}
body {
  max-width: 1000px;
}
```

```{r}
#1234567890#1234567890#1234567890#1234567890#1234567890#1234567890#1234567890#12
library(tidyverse)

ggplot2::diamonds

while (FALSE) {

}
```

## Introduction

[tidymodels](https://www.tidymodels.org/) is the new framework from
Max Kuhn, David Vaughan, and Julia Silge at RStudio. It's the successor to
the [caret](https://topepo.github.io/caret/) package, which was heavily
featured in Max Kuhn's book
[Applied Predictive Modeling](http://appliedpredictivemodeling.com/).

tidymodels promises a modular, extensible design for machine learning in R.
It also has a wonderful [website](https://www.tidymodels.org/) design
to help you get started as soon as possible, with a focus on examples and
case studies walking you through the components of the platform.

In this article, I want to translate concepts between tidymodels and
scikit-learn, Python's standard machine learning ecosystem. I hope this will
be useful for Python Data Scientists interested in R, and R users who want to
learn a little more about Python.

I'll cover the key machine learning workflows with scikit-learn and tidymodels
side-by-side, starting with preprocessing and feature engineering,
model tranining and cross-validation, as well as evaluation and comparison.

I think you'll find that while both frameworks have their own unique advantages,
there are many conceptual similarities that have helped me understand both
frameworks better.

## Testing columns

Lorem ipsum.

<div class="row">
<div class="col-md-6"><h2>R sagemaker</h2>
```{r}
#1234567890#1234567890#1234567890#1234567890#1234567890#1234567890#1234567890#12
library(tidyverse)

ggplot2::diamonds
```
</div>
<div class="col-md-6"><h2>AWS Sagemaker</h2>
```{python, eval=FALSE}
import pandas as pd
import numpy as np

input_data = 's3://sagemaker-sample-data-us-east-2/processing/census/census-income.csv'
df = pd.read_csv(input_data, nrows=1000)
df.head()
```
</div>
</div>

```{python, eval=FALSE}
def model_stats(model, X_test, y_test):
    prob = model.predict_proba(X_test)[:,1]
    pred = model.predict(X_test)

    print("roc: {:.2f}".format(roc_auc_score(y_test, prob)))
    print("prc: {:.2f}".format(average_precision_score(y_test, prob, pos_label=' 50000+.')))
    print()
    print(classification_report(y_test, pred))


model_stats(model, X_test, y_test)
```

```{r eval=FALSE}
library(tidyverse)

urchins <- read_csv("https://tidymodels.org/start/models/urchins.csv") %>%
  setNames(c("food_regime", "initial_volume", "width")) %>%
  mutate(food_regime = factor(food_regime, levels = c("Init", "Low", "High")))

urchins %>%
  group_by(food_regime) %>%
  summarise(
    across(everything(), mean), n = n()
  )

mean(urchins$width)
ggplot2::cut_interval(urchins$initial_volume)

lm(width ~ initial_volume * food_regime, data = urchins)

while (FALSE) {
}

18423 * 4
```


