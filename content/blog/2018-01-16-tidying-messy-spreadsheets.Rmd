---
title: Tidying Messy Spreadsheets
author: ~
date: '2018-01-17'
slug: tidying-messy-spreadsheets-dplyr
categories: []
tags: []
description: Tidying Messy Spreadsheets with dplyr and tidyr 
meta_img: /images/image.jpg
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, 
                      results='show', cache=FALSE, autodep=FALSE)
knitr::read_chunk("periodization-meta-analysis/tidier.R")
```

## Introduction

I want this post to be an introduction to cleaning and preparing a messy spreadsheet as part of a data science pipeline. Instead of presenting a final product, I'd like to emphasize exploration as a natural part of tidying. My approach will follow Hadley Wickham's tidy data principles outlined in his [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) paper. At the end, our data should satisfy these three characteristics:

- one variable per column

- one observation per row

- each type of observational unit forms a table

## Why Tidy?

In educational environments, data is served in a convenient format that is ready to be subjected to an array of algorithms from which we hope to drive some insight. But as Hadley explains in his [paper](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html), the reality is that the data scientists spend 80% of their time preparing data, and only 20% on the analysis itself. In his article, Hadley identifies tidy data as data that is ready to be analyzed by statistical programs. That's our goal here.

## The Data

I'll be tidying Greg Nuckol's spreadsheet of periodization studies on strength training found on his website [Strong By Science](https://www.strongerbyscience.com/periodization-data/). I choose this dataset because Greg shared it and encouraged others to analyze it, which I think is awesome. In a future blog post, I am going to conduct a Bayesian meta-analysis and share my results.

To be clear, the phrase "cleaning the dataset" is non-judgmental and I'm not trying to pick on Greg. When building this spreadsheet, Greg had more important things to do than to make it friendly for computers, especially when tidy data can sacrifice human readability and has various other [trade-offs](https://simplystatistics.org/2016/02/17/non-tidy-data/). 

I think these sorts of exercises develop fundamental skills for the aspiring data science and thought I would contribute my own example. I commend Greg's own analysis and his willingness to share his data, and I'm happy to go through the exercise of cleaning it up.

## Follow Along

The original data (downloaded 1/13/18 from Greg's website), the R script used to clean it, and the final tidy data can all be found in the [Github repo](https://github.com/tmastny/periodization-meta-analysis) I created to analyze Greg's work. 

I would encourage you download the data and follow along as I tidy up.

## Cleaning

Before we start coding, we actually need to *look* at the data. Since Greg shared his data on Google sheets, this was my first glimpse.

![](/blog/spreadsheet_pic.png)

Greg has only filled out the study's details for it's first occurrence in the spreadsheet. This makes it easy to read by adding white space between each study, but hard for the computer because there is an implicit spatial relation, which makes grouping and lookups difficult.

Luckily, we can easily correct this spatial dependence:^[Sometimes we can't easily fix spatial relations. For example, Excel pivot tables are notorious for the complicated use of whitespace and empty cells. For more advanced spreadsheet munging, check out [tidyxl](https://cran.r-project.org/web/packages/tidyxl/vignettes/tidyxl.html) package.] the first five variables always inherit the data from the previous row. We can code that as follows:

```{r}
library(tidyverse)
library(magrittr)
d <- readxl::read_excel('periodization-meta-analysis/Periodization Stuff.xlsx')
```

```{r spatial}
```

Next, we need to make sure our import from Excel to R was successful. I suggest printing the data frame in various ways. Here, I noticed that one column (mostly) full of numbers was read as a character vector.

```{r}
is.numeric(d$`Other 1 pre`)
is.character(d$`Other 1 pre`)
```

Manually looking through the data I see that there is a URL (accidentally?) stored in what should be a numeric variable:

```{r}
d[67,70]
```

But we can coerce our vector to numeric, which changes any characters to NAs.

```{r remove_url}
```

While exploring the file and figuring out how to clean it, I use filters and `head(d)` all the time. An easy way to improve the appearance of the console output is by hiding some of the less critical, text heavy columns using `dplyr::select`:

```{r}
d %<>%
  select(
    -`Measurements at 3+ time points?`, -Author, -`Study Title`,
    -`Participants (training status)`, -Age, -Sex, -`Length (weeks)`,
    -`Intensity Closest to 1RM test`, -`Volume Equated?`, -Issues)
```

This makes the output more manageable and informative. Run this early in your pipeline and either delete or comment it out when you are finished so you can preserve all the data.

Next, you should notice that the last 70 columns are all numeric, with names such as squat, bench, LBM, etc. This violates the second principle of tidy data: we should have *one observation per row*. Instead, we have one study for row, with multiple observations per study (squat, bench, LBM, etc) as columns. We need to gather the columns into one row:

```{r gather_variables}
```

Let's take a closer look at the variables we've gathered:

```{r}
unique(d$type)
```

Okay, first we can safely ignore anything with "ES"^[ES stands for effect size. We'll talk about that in the next blog post] or "%" in the name. Those are calculated, not observed quantities. 

Now I see a pattern in the rest of the column names. Each outcome is named something like **"[LBM/Bench/Squat] [Pre/Post/SD]"**. We actually have two different variables in one column, which violates the first tidy principle: *one variable per column*. We need to separate the columns so we have a variable for outcome types such as LBM, bench, squat, etc. and outcome measurements such as pre, post, and SD:

```{r split_columns}
```

This gives us

```{r}
head(d %>% select(Number, outcome_type, outcome_measurements, outcome))
```

As we can see, study number one didn't measurement LBM. We only care what each study did measure, so we can now remove all the NAs and change outcome to numeric.

```{r remove_na}
```

Now, let's narrow our attention to study number one. When in doubt, start small.

```{r}
head(d %>% filter(Number == 1) %>% select(-`Program Details`))
```

Taking a closer look at the new columns, I would contend that the `outcome_measurements` columns now violates principle one: *one variable per column*. For comparison, `outcome_type` is definitely one variable. It indicates what the study was actually measuring as an outcome. But `outcome_measurements` is a collection of three different variables, pre, post, and sd that measure some aspect of the `outcome_type`. Therefore, we need to separate into their own column.

Let's look at all the relevant data for study one.

```{r}
d %>% filter(Number == 1) %>% select(-N, -Number)
```

If we read closely, we can see that there were three unique protocols in study one. Each protocol has a unique pre, post, and sd measurement. We can exploit that grouped structure^[Check out [this](https://stackoverflow.com/questions/43259380/spread-with-duplicate-identifiers-using-tidyverse-and?noredirect=1&lq=1) stackoverflow answer for another example] by dividing the data into those groups and then spread the `outcome_measurements` to a column containing the `outcome` numeric. 

```{r}
d %>% 
  filter(Number == 1) %>%
  select(-N, -Number) %>%
  mutate_if(is.character, funs(factor(.))) %>%
  group_by(
    `Program Label`, `Program Details`, outcome_type, outcome_measurements) %>%
  spread(outcome_measurements, outcome)
```

This works perfect^[`mutate_if(is.character, funs(factor(.))) %>%` transforms each character column to a factor column for easy grouping. I believe `r-base` data frames do this automatically, but either `readxl` or `tibble` doesn't.]. Let's apply it to the rest of the data set (remembering to include the study number):

```{r group_by, error = TRUE}
```

We got an error, but we can work with this. It tells us where `spread` sees identical groups. This is most likely missing data, which tells us not every study is organized as nicely study one.  

```{r}
d[c(114, 115, 120, 121, 122, 123, 90, 91, 96, 97, 98, 99, 102, 103, 108, 109, 110, 111),]
```

As expected, lots of missing data. Seemingly important data such as Program Label, Details, and participants. There is a temptation just to toss it out, but let's go to the source to see if we are missing anything.

Referring back to the original data, it looks like the missing data is a strange encoding of some smaller muscles like elbow flexors and triceps. I'm going to exclude it, because it would probably take some manual data manipulation to fix. Also, in the next blog post I'm going to focus on squat and bench so it won't matter in the long run.

So let's try it without those rows:

```{r fix_data}
```

```{r group_by}
```

And we have no errors!

## Final Product

We could call `write.csv` on our dataset right now and be done with it. It would be shareable, but not repeatable. Not only that, but our dataset would be incomplete. Remember we deleted a bunch of columns with `dplyr:select` to make it easier to see on the command line. 

For these reasons, I would encourage you to translate your interactive commands into a standalone `.R` script that exists in an isolated environment only containing the original data, such as [the one](https://github.com/tmastny/periodization-meta-analysis/blob/master/tidier.R) I made for this blog post. The isolated environment eliminates forgotten dependencies in the interactive working environment, an `.R` script solves the repeatibility problem, and posting on Github improves our sharing.


## Conclusion

Figuring out how to clean a dataset should be interactive and exploratory. Trying things out and getting error messages points you in the next direction. [Don't wring your hands: start small](https://twitter.com/JennyBryan/status/952285541617123328). But simultaneously, save what works. Iterate on what you have. And keep the exploration environment and product environment separate so you aren't afraid to break things, and you have a baseline to return to when you do.

This synergy between exploration and iteration is universal in data science, and it was fun to share it with you in this blog post. 

